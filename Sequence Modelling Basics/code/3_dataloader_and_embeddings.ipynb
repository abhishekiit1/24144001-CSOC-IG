{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e815a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d284982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique polarity values: [0 1]\n",
      "Polarity dtype: int64\n",
      "                                      cleaned_review  \\\n",
      "0  the light bulb does not light anything, has a ...   \n",
      "1  i purchased the flowtron bf 190 replacement bu...   \n",
      "2  it doesn't work well in the hamilton beech sin...   \n",
      "3  it took me forever to get through this book. i...   \n",
      "4  this was a little of a deception, is smaller t...   \n",
      "\n",
      "                      title  polarity  \n",
      "0       PIAA SUPER LED BULB         0  \n",
      "1               unsatisfied         0  \n",
      "2  Refillable Coffee Filter         0  \n",
      "3         Not to our liking         0  \n",
      "4               Not so good         0  \n",
      "Dataset size: 799998\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\abhis\\OneDrive\\Desktop\\week4\\Sequence Modelling Basics\\data\\cleaned_sampled_train.csv\")\n",
    "df = df.dropna(subset=['cleaned_review'])  # Drop rows with missing review text\n",
    "df = df[df['cleaned_review'].apply(lambda x: isinstance(x, str))]  # Ensure all are strings\n",
    "\n",
    "df['polarity'] = df['polarity'].map({1: 0, 2: 1})  # map labels to 0 and 1 for BCE loss\n",
    "# Check unique values after mapping\n",
    "print(\"Unique polarity values:\", df['polarity'].unique())\n",
    "\n",
    "# Check dtype (should be int or float before tensor conversion)\n",
    "print(\"Polarity dtype:\", df['polarity'].dtype)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Dataset size: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f4dd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower().strip().split()\n",
    "    return []  # Return empty list if text is not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a5f7c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [the, light, bulb, does, not, light, anything,...\n",
      "1    [i, purchased, the, flowtron, bf, 190, replace...\n",
      "2    [it, doesn't, work, well, in, the, hamilton, b...\n",
      "3    [it, took, me, forever, to, get, through, this...\n",
      "4    [this, was, a, little, of, a, deception,, is, ...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenizer(text):\n",
    "    text = text.lower().strip()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['cleaned_review'].apply(simple_tokenizer)\n",
    "\n",
    "print(df['tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181ed0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (including PAD & UNK): 20000\n",
      "0    [2, 359, 3692, 107, 16, 359, 3498, 40, 5, 1024...\n",
      "1    [4, 201, 2, 1, 1, 1, 717, 3692, 19, 210, 10101...\n",
      "2    [10, 160, 130, 91, 11, 2, 6605, 1, 521, 2809, ...\n",
      "3    [10, 297, 58, 1814, 6, 51, 146, 8, 131, 10, 10...\n",
      "4    [8, 14, 5, 101, 7, 5, 1, 9, 1122, 64, 4, 1, 39...\n",
      "Name: indexed_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build vocabulary from tokens\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Set vocab size limit (e.g., 20,000 most frequent tokens)\n",
    "vocab_size = 20000\n",
    "most_common_tokens = token_counts.most_common(vocab_size - 2)  # reserve 2 for PAD and UNK\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Build word to index dict\n",
    "word2idx = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for i, (word, _) in enumerate(most_common_tokens, start=2):\n",
    "    word2idx[word] = i\n",
    "\n",
    "print(f\"Vocabulary size (including PAD & UNK): {len(word2idx)}\")\n",
    "\n",
    "# Map tokens to indices, replace unknown tokens with UNK index\n",
    "def tokens_to_indices(tokens):\n",
    "    return [word2idx.get(token, word2idx[UNK_TOKEN]) for token in tokens]\n",
    "\n",
    "df['indexed_tokens'] = df['tokens'].apply(tokens_to_indices)\n",
    "\n",
    "print(df['indexed_tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b4f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [2, 359, 3692, 107, 16, 359, 3498, 40, 5, 1024...\n",
      "1    [4, 201, 2, 1, 1, 1, 717, 3692, 19, 210, 10101...\n",
      "2    [10, 160, 130, 91, 11, 2, 6605, 1, 521, 2809, ...\n",
      "3    [10, 297, 58, 1814, 6, 51, 146, 8, 131, 10, 10...\n",
      "4    [8, 14, 5, 101, 7, 5, 1, 9, 1122, 64, 4, 1, 39...\n",
      "Name: padded_tokens, dtype: object\n",
      "Padded sequences shape example: (150,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 150  # You can adjust this based on your review length stats\n",
    "\n",
    "# Pad sequences with 0 (PAD token index)\n",
    "df['padded_tokens'] = list(pad_sequences(df['indexed_tokens'], maxlen=max_len, padding='post', truncating='post'))\n",
    "\n",
    "print(df['padded_tokens'].head())\n",
    "print(f\"Padded sequences shape example: {df['padded_tokens'].iloc[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c76e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shapes:\n",
      "Texts: torch.Size([128, 150])\n",
      "Labels: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(np.array(texts), dtype=torch.long)  # fix for speed\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Prepare Dataset\n",
    "dataset = ReviewDataset(df['padded_tokens'].tolist(), df['polarity'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Inspect a sample batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(\"Sample batch shapes:\")\n",
    "print(\"Texts:\", sample_batch[0].shape)\n",
    "print(\"Labels:\", sample_batch[1].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

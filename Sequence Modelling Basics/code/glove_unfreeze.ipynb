{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17195f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d31b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique polarity values: [0 1]\n",
      "Polarity dtype: int64\n",
      "                                      cleaned_review  \\\n",
      "0  the light bulb does not light anything, has a ...   \n",
      "1  i purchased the flowtron bf 190 replacement bu...   \n",
      "2  it doesn't work well in the hamilton beech sin...   \n",
      "3  it took me forever to get through this book. i...   \n",
      "4  this was a little of a deception, is smaller t...   \n",
      "\n",
      "                      title  polarity  \n",
      "0       PIAA SUPER LED BULB         0  \n",
      "1               unsatisfied         0  \n",
      "2  Refillable Coffee Filter         0  \n",
      "3         Not to our liking         0  \n",
      "4               Not so good         0  \n",
      "Dataset size: 799998\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\abhis\\OneDrive\\Desktop\\week4\\Sequence Modelling Basics\\data\\cleaned_sampled_train.csv\")\n",
    "df = df.dropna(subset=['cleaned_review'])  # Drop rows with missing review text\n",
    "df = df[df['cleaned_review'].apply(lambda x: isinstance(x, str))]  # Ensure all are strings\n",
    "\n",
    "df['polarity'] = df['polarity'].map({1: 0, 2: 1})  # map labels to 0 and 1 for BCE loss\n",
    "# Check unique values after mapping\n",
    "print(\"Unique polarity values:\", df['polarity'].unique())\n",
    "\n",
    "# Check dtype (should be int or float before tensor conversion)\n",
    "print(\"Polarity dtype:\", df['polarity'].dtype)\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Dataset size: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94578db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower().strip().split()\n",
    "    return []  # Return empty list if text is not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ecd681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [the, light, bulb, does, not, light, anything,...\n",
      "1    [i, purchased, the, flowtron, bf, 190, replace...\n",
      "2    [it, doesn't, work, well, in, the, hamilton, b...\n",
      "3    [it, took, me, forever, to, get, through, this...\n",
      "4    [this, was, a, little, of, a, deception,, is, ...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenizer(text):\n",
    "    text = text.lower().strip()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['cleaned_review'].apply(simple_tokenizer)\n",
    "\n",
    "print(df['tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d05611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (including PAD & UNK): 20000\n",
      "0    [2, 359, 3692, 107, 16, 359, 3498, 40, 5, 1024...\n",
      "1    [4, 201, 2, 1, 1, 1, 717, 3692, 19, 210, 10101...\n",
      "2    [10, 160, 130, 91, 11, 2, 6605, 1, 521, 2809, ...\n",
      "3    [10, 297, 58, 1814, 6, 51, 146, 8, 131, 10, 10...\n",
      "4    [8, 14, 5, 101, 7, 5, 1, 9, 1122, 64, 4, 1, 39...\n",
      "Name: indexed_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build vocabulary from tokens\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Set vocab size limit (e.g., 20,000 most frequent tokens)\n",
    "vocab_size = 20000\n",
    "most_common_tokens = token_counts.most_common(vocab_size - 2)  # reserve 2 for PAD and UNK\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Build word to index dict\n",
    "word2idx = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for i, (word, _) in enumerate(most_common_tokens, start=2):\n",
    "    word2idx[word] = i\n",
    "\n",
    "print(f\"Vocabulary size (including PAD & UNK): {len(word2idx)}\")\n",
    "\n",
    "# Map tokens to indices, replace unknown tokens with UNK index\n",
    "def tokens_to_indices(tokens):\n",
    "    return [word2idx.get(token, word2idx[UNK_TOKEN]) for token in tokens]\n",
    "\n",
    "df['indexed_tokens'] = df['tokens'].apply(tokens_to_indices)\n",
    "\n",
    "print(df['indexed_tokens'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfd22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [2, 359, 3692, 107, 16, 359, 3498, 40, 5, 1024...\n",
      "1    [4, 201, 2, 1, 1, 1, 717, 3692, 19, 210, 10101...\n",
      "2    [10, 160, 130, 91, 11, 2, 6605, 1, 521, 2809, ...\n",
      "3    [10, 297, 58, 1814, 6, 51, 146, 8, 131, 10, 10...\n",
      "4    [8, 14, 5, 101, 7, 5, 1, 9, 1122, 64, 4, 1, 39...\n",
      "Name: padded_tokens, dtype: object\n",
      "Padded sequences shape example: (150,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 150  # You can adjust this based on your review length stats\n",
    "\n",
    "# Pad sequences with 0 (PAD token index)\n",
    "df['padded_tokens'] = list(pad_sequences(df['indexed_tokens'], maxlen=max_len, padding='post', truncating='post'))\n",
    "\n",
    "print(df['padded_tokens'].head())\n",
    "print(f\"Padded sequences shape example: {df['padded_tokens'].iloc[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b19185f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 458. MiB for an array with shape (799998, 150) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Prepare Dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mReviewDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpadded_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolarity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create DataLoader\u001b[39;00m\n\u001b[0;32m     21\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m, in \u001b[0;36mReviewDataset.__init__\u001b[1;34m(self, texts, labels)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, labels):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# fix for speed\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 458. MiB for an array with shape (799998, 150) and data type int32"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(np.array(texts), dtype=torch.long)  # fix for speed\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Prepare Dataset\n",
    "dataset = ReviewDataset(df['padded_tokens'].tolist(), df['polarity'])\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Inspect a sample batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(\"Sample batch shapes:\")\n",
    "print(\"Texts:\", sample_batch[0].shape)\n",
    "print(\"Labels:\", sample_batch[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c41f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['polarity'])\n",
    "\n",
    "# Custom Dataset\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(np.array(texts), dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReviewDataset(train_df['padded_tokens'].tolist(), train_df['polarity'])\n",
    "val_dataset = ReviewDataset(val_df['padded_tokens'].tolist(), val_df['polarity'])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sample batch shape (X):\", sample_batch[0].shape)\n",
    "print(\"Sample batch shape (y):\", sample_batch[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63719c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d10e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4fef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26141d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dc590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
